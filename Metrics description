{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOUhLwcZ6iCj5uG4ABEGKSr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Описание выбранных метрик\n","Для данной лабораторной работы я выбрал следующие метрики качества.\n"],"metadata":{"id":"6D8lknsOhNRP"}},{"cell_type":"markdown","source":["## 1. Регрессия\n","\n","Для оценки качества моделей регрессии в работе используются метрики **MAE**, **RMSE** и коэффициент детерминации **R²**. Выбор нескольких метрик обусловлен тем, что каждая из них по-разному отражает точность предсказаний и чувствительность к ошибкам.\n","\n","### MAE (Mean Absolute Error, средняя абсолютная ошибка)\n","\n","$$\n","MAE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|\n","$$\n","\n","MAE показывает среднюю величину ошибки между предсказанными и истинными значениями в тех же единицах, что и целевая переменная. Метрика устойчива к выбросам и легко интерпретируется.\n","\n","### RMSE (Root Mean Squared Error, среднеквадратичная ошибка)\n","\n","$$\n","RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n","$$\n","\n","RMSE сильнее штрафует большие ошибки, так как использует квадратичное отклонение. Если RMSE значительно больше MAE, это указывает на наличие крупных ошибок.\n","\n","Использование совместно MAE и RMSE позволяет понять, есть ли у модели отдельные крупные ошибки: если RMSE заметно больше MAE, это указывает на наличие значительных выбросов в ошибках.\n","\n","### Коэффициент детерминации R²\n","\n","$$\n","R^{2} = 1 \\;-\\;\n","\\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n","     {\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n","$$\n","\n","R² даёт относительную оценку качества модели по сравнению с простой базовой моделью. Значения ближе к 1 соответствуют лучшему качеству модели.\n","\\\n","\\\n","Совместное применение метрик MAE, RMSE и коэффициента детерминации R² позволяет получить всестороннюю оценку качества регрессионной модели. Каждая из этих метрик отражает различные аспекты точности предсказаний, и использование только одной из них даёт неполное представление о работе модели.\n","\n"],"metadata":{"id":"TMTqnLCrhMcn"}},{"cell_type":"markdown","source":["## 2. Классификация\n","\n","Для оценки качества моделей классификации используются метрики **accuracy**, **F1** и **ROC-AUC**. Совместное применение этих показателей позволяет получить полное представление о работе модели.\n","\n","### Accuracy (доля правильных предсказаний)\n","\n","$$\n","Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n","$$\n","\n","Accuracy удобна как базовая метрика, однако может быть недостаточно информативной при дисбалансе классов.\n","\n","### F1-мера\n","\n","$$\n","F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\n","$$\n","\n","F1 учитывает одновременно точность и полноту, что делает её особенно полезной при неравномерном распределении классов.\n","\n","### ROC-AUC\n","\n","$$\n","AUC = \\int_{0}^{1} TPR(FPR) \\, d(FPR)\n","$$\n","\n","ROC-AUC оценивает качество ранжирования модели, не завися от выбранного порога классификации. Чем ближе AUC к 1, тем лучше модель различает классы.\n","\n"],"metadata":{"id":"S0SFky9bh0Y8"}}]}